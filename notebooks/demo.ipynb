{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from importlib import reload\n",
    "import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from mfishtoolspy import mfishtoolspy as mft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options for cluster grouping\n",
    "ops = {\n",
    "    # 'panel_name': 'Pan-neuronal', # GABAergic or Glutamatergic or Pan-neuronal\n",
    "    'panel_name': 'GABAergic', # GABAergic or Glutamatergic or Pan-neuronal\n",
    "    # 'full_panel_size': 28,\n",
    "    'full_panel_size': 22,\n",
    "    # 'starting_genes': [\"Gad2\",\"Slc17a7\",\"Pvalb\",\"Sst\",\"Vip\",\"Cck\",\"Tac1\",\"Npy\",\"Crh\",\"Necab1\",\"Ptprt\",\"Kirrel3\",\"Penk\",\"Hpse\",\"Calb2\",\"Chodl\"],\n",
    "    'starting_genes': [\"Gad2\",\"Slc17a7\",\"Pvalb\",\"Sst\",\"Vip\", \"Sncg\", \"Lamp5\", \"Npy\", \"Calb2\", \"Tac1\", \"Cck\", \"Ndnf\"],\n",
    "    'layer_1234_filter': True,\n",
    "    'GABAergic_group_level': 'cluster', # class, subclass, supertype, or cluster\n",
    "    'GABAergic_mapping_level': 'cluster',\n",
    "    'Glutamatergic_group_level': 'subclass', # class, subclass, supertype, or cluster\n",
    "    'Glutamatergic_mapping_level': 'subclass',\n",
    "    'GABAergic_other_group_level': 'subclass', # class, subclass, supertype, or cluster\n",
    "    'GABAergic_other_mapping_level': 'subclass',\n",
    "    'Glutamatergic_other_group_level': 'class', # class, subclass, supertype, or cluster\n",
    "    'Glutamatergic_other_mapping_level': 'class',\n",
    "    'blend_supertypes': False,  # Don't know if I'm going to keep this\n",
    "    'remove_redundant_genes': False, # from the starting_genes list\n",
    "    'remove_redundant_genes_threshold': 0.95, # threshold for removing redundant genes from normalized accuracy\n",
    "    'L1234_layer_threshold': 0.15,\n",
    "    'L6_layer_threshold': 0.7,\n",
    "    'L1234_labels': ['L1', 'L1-L2/3', 'L1-L4', 'L2/3', 'L2/3-L4', 'L4'],\n",
    "    'L6_labels': ['L6', 'L6b'],\n",
    "    'max_on': 5000,\n",
    "    'max_off': 1129,\n",
    "    'min_on': 10,\n",
    "    'max_fraction_on_clusters': 0.5,\n",
    "    'num_subsample': 50,\n",
    "}\n",
    "\n",
    "# paths to the data\n",
    "data_folder = Path(r'\\\\allen\\programs\\mindscope\\workgroups\\learning\\jinho\\gene_panel_selection\\data\\mouse_VISp_gene_expression_matrices_2018-06-14')\n",
    "output_folder = Path(r'\\\\allen\\programs\\mindscope\\workgroups\\learning\\jinho\\gene_panel_selection\\results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess options\n",
    "if 'GABAergic' in ops['panel_name']:\n",
    "    ops['keep_class'] = ['GABAergic']\n",
    "elif 'Glutamatergic' in ops['panel_name']:\n",
    "    ops['keep_class'] = ['Glutamatergic']\n",
    "elif 'Pan-neuronal' in ops['panel_name']:\n",
    "    ops['keep_class'] = ['GABAergic', 'Glutamatergic']\n",
    "else:\n",
    "    raise ValueError('panel_name must be GABAergic, Glutamatergic, or Pan-neuronal')\n",
    "\n",
    "level_hierarchy = {'class': 0, 'subclass': 1, 'supertype': 2, 'cluster': 3}\n",
    "assert level_hierarchy[ops['GABAergic_group_level']] <= level_hierarchy[ops['GABAergic_mapping_level']]\n",
    "assert level_hierarchy[ops['GABAergic_other_group_level']] <= level_hierarchy[ops['GABAergic_other_mapping_level']]\n",
    "assert level_hierarchy[ops['Glutamatergic_group_level']] <= level_hierarchy[ops['Glutamatergic_mapping_level']]\n",
    "assert level_hierarchy[ops['Glutamatergic_other_group_level']] <= level_hierarchy[ops['Glutamatergic_other_mapping_level']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinho.kim\\AppData\\Local\\Temp\\ipykernel_42460\\2181021420.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  annotation['supertype_label'] = annotation.cluster_label.map(supertype.supertype)\n"
     ]
    }
   ],
   "source": [
    "# load and pre-process data (takes about 1 min)\n",
    "\n",
    "# read annotation\n",
    "annotation = pd.read_feather(data_folder / 'anno.feather')\n",
    "# read data\n",
    "# TODO: check where this data is coming from. Values are similar to cpm but not exactly the same\n",
    "cpm_data = pd.read_feather(data_folder / 'exon_cpm.feather')\n",
    "tpm_data = pd.read_feather(data_folder / 'exon_tpm.feather')\n",
    "\n",
    "annotation.set_index('sample_id', inplace=True, drop=True)\n",
    "# data.set_index('gene', inplace=True, drop=True) # only necessary with data_t.feather\n",
    "\n",
    "# preprocessing\n",
    "# Removing 'X\" in column? from Hannah's code. Don't know when this happens, but leave them here just in case.\n",
    "if 'X' in cpm_data.columns:\n",
    "    print('Dropping \"X\" column from data')\n",
    "    cpm_data = cpm_data.drop(columns=['X'])\n",
    "if 'X' in tpm_data.columns:\n",
    "    print('Dropping \"X\" column from data')\n",
    "    tpm_data = tpm_data.drop(columns=['X']) \n",
    "if 'X' in annotation.columns:\n",
    "    print('Dropping \"X\" column fro annotation')\n",
    "    annotation = annotation.drop(columns=['X'])\n",
    "\n",
    "# change the row order of annotation to match the order of columns in data\n",
    "# annotation = annotation.loc[data.columns]  # don't need this, but add assert statement to check\n",
    "assert np.all(annotation.index.values == tpm_data.columns.values)\n",
    "assert np.all(annotation.index.values == cpm_data.columns.values)\n",
    "\n",
    "# data conversion to log2\n",
    "tpm_log2 = np.log2(tpm_data + 1)\n",
    "cpm_log2 = np.log2(cpm_data + 1)\n",
    "\n",
    "# read supertype information\n",
    "# TODO: re-define supertype (will be addressed in another notebook)\n",
    "supertype_folder = Path('//allen/programs/mindscope/workgroups/omfish/hannahs/mfish_project/gene_panels')\n",
    "supertype_fn = supertype_folder / 'tasic2018_supertypes_manual_v2.xlsx'\n",
    "sheet_name = 'all_supertypes_v2'\n",
    "supertype = pd.read_excel(supertype_fn, sheet_name=sheet_name)\n",
    "supertype.rename(columns={'Cell Type': 'cell_type', 'Supertype': 'supertype'}, inplace=True)\n",
    "supertype.cell_type = supertype.cell_type.str.replace('\\xa0', ' ')\n",
    "supertype.supertype = supertype.supertype.str.replace('\\xa0', ' ')\n",
    "assert np.all([ct in annotation['cluster_label'].values for ct in supertype.cell_type.values])\n",
    "supertype.set_index('cell_type', inplace=True, drop=True)\n",
    "\n",
    "annotation['supertype_label'] = annotation.cluster_label.map(supertype.supertype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jinho.kim\\AppData\\Local\\Temp\\ipykernel_42460\\2259671364.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  annotation.loc[annotation.class_label == 'GABAergic', 'group_label'] = group_label_list\n",
      "C:\\Users\\jinho.kim\\AppData\\Local\\Temp\\ipykernel_42460\\2259671364.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  annotation.loc[annotation.class_label == 'GABAergic', 'mapping_label'] = mapping_label_list\n"
     ]
    }
   ],
   "source": [
    "# assign mappings and groups and update the ops dictionary\n",
    "\n",
    "group_label = f'{ops[\"Glutamatergic_group_level\"]}_label'\n",
    "mapping_label = f'{ops[\"Glutamatergic_mapping_level\"]}_label'\n",
    "temp_annotation = annotation.query('class_label==\"Glutamatergic\"')\n",
    "\n",
    "# filtering and assigning group_label\n",
    "# Group means target clustering that I want to classify eventually\n",
    "# There is also mapping_label, to which I want to match first before calculating classification accuracy\n",
    "# E.g., match individual samples to cluster level but then classify them at supertype level\n",
    "# E.g., classification can happen in mixed level, some at the cluster level while others in subclass level\n",
    "\n",
    "# If \"other\" group and mapping levels are different from group and mapping levels\n",
    "# then assign relevant labels to \"other\" group and mapping levels\n",
    "#   \"other\" is relevant only when filtering by the layers\n",
    "#   It means the rest of groups of mappings that can be within the imaging regime but we want to exclude from further analysis\n",
    "\n",
    "# Filtering based on layer abundance\n",
    "# TODO: test thresholds. \n",
    "# TODO: Better to do this from merFISH data\n",
    "\n",
    "keep_groups = []\n",
    "keep_mappings = []\n",
    "other_groups = []\n",
    "other_mappings = []\n",
    "\n",
    "if 'Glutamatergic' in ops['keep_class']:\n",
    "    # Assign group and mapping labels\n",
    "    group_level_label = f'{ops[\"Glutamatergic_group_level\"]}_label'\n",
    "    group_label_list = [f'Glutamatergic {gl}' for gl in annotation.loc[annotation.class_label == 'Glutamatergic', group_level_label]] # To disambiguate from GABAergic groups\n",
    "    annotation.loc[annotation.class_label == 'Glutamatergic', 'group_label'] = group_label_list\n",
    "    mapping_level_label = f'{ops[\"Glutamatergic_mapping_level\"]}_label'\n",
    "    mapping_label_list = [f'Glutamatergic {ml}' for ml in annotation.loc[annotation.class_label == 'Glutamatergic', mapping_label]] # To disambiguate from GABAergic mappings\n",
    "    annotation.loc[annotation.class_label == 'Glutamatergic', 'mapping_label'] = mapping_label_list\n",
    "    \n",
    "    # Assign keep and other groups.\n",
    "    # If lower than class level, then consider layer filtering (L1234 only for now)\n",
    "    if ops['Glutamatergic_group_level'] == 'class':\n",
    "        keep_groups += ['Glutamatergic']\n",
    "    else:\n",
    "        temp_annotation = annotation.query('class_label==\"Glutamatergic\"')\n",
    "        if ops['layer_1234_filter']:\n",
    "            keep_groups += [gl for gl in temp_annotation['group_label'].unique().tolist() if gl.split(' ')[1] in ['L2/3', 'L4']] # Have class label at the beginning\n",
    "            # Process \"other\" groups\n",
    "            other_group_level_label = f'{ops[\"Glutamatergic_other_group_level\"]}_label'\n",
    "            temp_other_groups = [gl for gl in temp_annotation['group_label'].unique().tolist() if gl.split(' ')[1] in ['L5', 'NP']] # Have class label at the beginning\n",
    "            if other_group_level_label == group_level_label: # no need to change group labels\n",
    "                other_groups += temp_other_groups\n",
    "            else:\n",
    "                # Add L5 after class label\n",
    "                other_group_labels = [f'Glutamatergic L5 {gl}' for gl in annotation[annotation['group_label'].isin(temp_other_groups)][other_group_level_label].values]\n",
    "                annotation.loc[annotation['group_label'].isin(temp_other_groups), 'group_label'] = other_group_labels\n",
    "                other_groups += np.unique(other_group_labels).tolist()\n",
    "        else:\n",
    "            keep_groups += temp_annotation['group_label'].unique().tolist()\n",
    "    # Assign keep and other mappings.\n",
    "    if ops['Glutamatergic_mapping_level'] == 'class':\n",
    "        keep_mappings += ['Glutamatergic']\n",
    "    else:\n",
    "        temp_annotation = annotation.query('class_label==\"Glutamatergic\"')\n",
    "        if ops['layer_1234_filter']:\n",
    "            keep_mappings += [ml for ml in temp_annotation['mapping_label'].unique().tolist() if ml.split(' ')[1] in ['L2/3', 'L4']] # Have class label at the beginning\n",
    "            # Process \"other\" groups\n",
    "            other_mapping_level_label = f'{ops[\"Glutamatergic_other_mapping_level\"]}_label'\n",
    "            temp_other_mappings = [ml for ml in temp_annotation['mapping_label'].unique().tolist() if ml.split(' ')[1] in ['L5', 'NP']] # Have class label at the beginning\n",
    "            if other_mapping_level_label == mapping_level_label: # no need to change mapping labels\n",
    "                other_mappings += temp_other_mappings\n",
    "            else:\n",
    "                # Add L5 after class label\n",
    "                other_mapping_labels = [f'Glutamatergic L5 {ml}' for ml in annotation[annotation['mapping_label'].isin(temp_other_mappings)][other_mapping_level_label].values]\n",
    "                annotation.loc[annotation['mapping_label'].isin(temp_other_mappings), 'mapping_label'] = other_mapping_labels\n",
    "                other_mappings += np.unique(other_mapping_labels).tolist()\n",
    "        else:\n",
    "            keep_mappings += temp_annotation['mapping_label'].unique().tolist()\n",
    "\n",
    "# Same for GABAergic\n",
    "# Except for filtering, now we are using scRNAseq layer-enriched data with thresholds\n",
    "if 'GABAergic' in ops['keep_class']:\n",
    "    # Assign group and mapping labels\n",
    "    group_level_label = f'{ops[\"GABAergic_group_level\"]}_label'\n",
    "    group_label_list = [f'GABAergic {gl}' for gl in annotation.loc[annotation.class_label == 'GABAergic', group_level_label]] # To disambiguate from Glutamatergic groups\n",
    "    annotation.loc[annotation.class_label == 'GABAergic', 'group_label'] = group_label_list\n",
    "    mapping_level_label = f'{ops[\"GABAergic_mapping_level\"]}_label'\n",
    "    mapping_label_list = [f'GABAergic {ml}' for ml in annotation.loc[annotation.class_label == 'GABAergic', mapping_level_label]] # To disambiguate from Glutamatergic mappings\n",
    "    annotation.loc[annotation.class_label == 'GABAergic', 'mapping_label'] = mapping_label_list\n",
    "    \n",
    "    # Assign keep and other groups.\n",
    "    # If lower than class level, then consider layer filtering (L1234 only for now)\n",
    "    # Also need to name them different (adding L5 in front of the group and mapping labels)\n",
    "    if ops['GABAergic_group_level'] == 'class':\n",
    "        keep_groups += ['GABAergic']\n",
    "    else:\n",
    "        temp_annotation = annotation.query('class_label==\"GABAergic\"')\n",
    "        if ops['layer_1234_filter']:\n",
    "            # Filtering process based on the layer abundance\n",
    "            layer_df = annotation.query('class_label==\"GABAergic\"')[['layer_label', 'cluster_label']].copy()\n",
    "            layer_table = layer_df.groupby(['layer_label', 'cluster_label']).size().unstack(fill_value=0)\n",
    "            prop_table = layer_table.div(layer_table.sum(axis=0), axis=1)\n",
    "            L1234_prop_sum = prop_table.loc[ops['L1234_labels']].sum(axis=0)\n",
    "            L1234_inh_types = set(L1234_prop_sum[L1234_prop_sum >= ops['L1234_layer_threshold']].index.values)\n",
    "            not_L1234_inh_types = set(layer_df.cluster_label).difference(L1234_inh_types)\n",
    "            L6_prop_sum = prop_table.loc[ops['L6_labels']].sum(axis=0)\n",
    "            L6_inh_types = set(L6_prop_sum[L6_prop_sum >= ops['L6_layer_threshold']].index.values)\n",
    "            L5_inh_types = not_L1234_inh_types.difference(L6_inh_types)\n",
    "            # L1234_inh_types are going to be kept\n",
    "            # L5_inh_types are going to be \"other\"\n",
    "            # Ignore L6_inh_types (assume they won't be imaged)\n",
    "            keep_annotation = temp_annotation[temp_annotation['cluster_label'].isin(L1234_inh_types)]\n",
    "            other_annotation = temp_annotation[temp_annotation['cluster_label'].isin(L5_inh_types)]\n",
    "            \n",
    "            keep_groups += keep_annotation['group_label'].unique().tolist()\n",
    "\n",
    "            # Process \"other\" groups\n",
    "            other_group_level_label = f'{ops[\"GABAergic_other_group_level\"]}_label'\n",
    "            temp_other_groups = other_annotation['group_label'].unique().tolist()\n",
    "            if other_group_level_label == group_level_label: # no need to change group labels\n",
    "                other_groups += temp_other_groups\n",
    "            else:\n",
    "                # Add L5 after class label\n",
    "                other_group_labels = [f'GABAergic L5 {gl}' for gl in annotation[annotation['group_label'].isin(temp_other_groups)][other_group_level_label].values]\n",
    "                annotation.loc[annotation['group_label'].isin(temp_other_groups), 'group_label'] = other_group_labels\n",
    "                other_groups += np.unique(other_group_labels).tolist()\n",
    "        else:\n",
    "            keep_groups += temp_annotation['group_label'].unique().tolist()\n",
    "    # Assign keep and other mappings.\n",
    "    if ops['GABAergic_mapping_level'] == 'class':\n",
    "        keep_mappings += ['GABAergic']\n",
    "    else:\n",
    "        temp_annotation = annotation.query('class_label==\"GABAergic\"')\n",
    "        if ops['layer_1234_filter']:\n",
    "            # Filtering process should have been done already in the above if clause\n",
    "            # keep_annotation and other_annotation are already defined\n",
    "            keep_mappings += keep_annotation['mapping_label'].unique().tolist()\n",
    "            \n",
    "            # Process \"other\" groups\n",
    "            other_mapping_level_label = f'{ops[\"GABAergic_other_mapping_level\"]}_label'\n",
    "            temp_other_mappings = other_annotation['mapping_label'].unique().tolist()\n",
    "            if other_mapping_level_label == mapping_level_label: # no need to change mapping labels\n",
    "                other_mappings += temp_other_mappings\n",
    "            else:\n",
    "                # Add L5 after class label\n",
    "                other_mapping_labels = [f'GABAergic L5 {ml}' for ml in annotation[annotation['mapping_label'].isin(temp_other_mappings)][other_mapping_level_label].values]\n",
    "                annotation.loc[annotation['mapping_label'].isin(temp_other_mappings), 'mapping_label'] = other_mapping_labels\n",
    "                other_mappings += np.unique(other_mapping_labels).tolist()\n",
    "        else:\n",
    "            keep_mappings += temp_annotation['mapping_label'].unique().tolist()\n",
    "\n",
    "# assign cluster to nan mapping labels\n",
    "# for filtering using \"off clusters\" information (e.g., glial cells)\n",
    "annotation.loc[annotation['mapping_label']=='nan', 'mapping_label'] = annotation.loc[annotation['mapping_label']=='nan', 'cluster_label'] \n",
    "\n",
    "\n",
    "ops['keep_mappings'] = keep_mappings\n",
    "ops['other_mappings'] = other_mappings\n",
    "ops['keep_groups'] = keep_groups\n",
    "ops['other_groups'] = other_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate proportions and medians for mapping labels and for group labels\n",
    "# Takes about 1 min\n",
    "expr_thresh = 1\n",
    "# make data_log2 to have another level of columns with matching cluster names per cell ID\n",
    "tpm_log2_cluster = tpm_log2.copy().T\n",
    "gene_column_names = list(tpm_log2_cluster.columns)\n",
    "assert np.all(tpm_log2.columns == annotation.index.values)\n",
    "# groupby cluster and calculate median and proportion\n",
    "tpm_log2_cluster['mapping_label'] = annotation['mapping_label']\n",
    "tpm_log2_cluster['group_label'] = annotation['group_label']\n",
    "tpm_median_per_mapping = tpm_log2_cluster[gene_column_names + ['mapping_label']].groupby('mapping_label').median().T\n",
    "tpm_prop_expr_mapping = tpm_log2_cluster[gene_column_names + ['mapping_label']].groupby('mapping_label').apply(lambda x: (x > expr_thresh).mean(axis=0)).T\n",
    "tpm_median_per_group = tpm_log2_cluster[gene_column_names + ['group_label']].groupby('group_label').median().T\n",
    "tpm_prop_expr_group = tpm_log2_cluster[gene_column_names + ['group_label']].groupby('group_label').apply(lambda x: (x > expr_thresh).mean(axis=0)).T\n",
    "assert np.all(tpm_prop_expr_mapping.index.values == tpm_median_per_mapping.index.values)\n",
    "assert np.all(tpm_prop_expr_group.index.values == tpm_median_per_group.index.values)\n",
    "assert np.all(tpm_prop_expr_mapping.index.values == tpm_log2.index.values)\n",
    "assert np.all(tpm_prop_expr_group.index.values == tpm_log2.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806 total genes pass constraints prior to binary score calculation.\n"
     ]
    }
   ],
   "source": [
    "tpm_summary_data = 2**tpm_median_per_mapping - 1\n",
    "run_genes, filtered_out_genes = \\\n",
    "    mft.filter_panel_genes(tpm_summary_data,\n",
    "                            prop_expr=tpm_prop_expr_mapping,\n",
    "                            on_clusters=keep_mappings + other_mappings,\n",
    "                            starting_genes=ops['starting_genes'],\n",
    "                            off_clusters=list(annotation.query('class_label==\"Non-Neuronal\"').mapping_label.unique()),\n",
    "                            max_on=ops['max_on'],\n",
    "                            max_off=ops['max_off'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mfishtoolspy.mfishtoolspy' from 'c:\\\\users\\\\jinho.kim\\\\github\\\\mfishtoolspy\\\\mfishtoolspy\\\\mfishtoolspy.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mfishtoolspy.mfishtoolspy' from 'c:\\\\users\\\\jinho.kim\\\\github\\\\mfishtoolspy\\\\mfishtoolspy\\\\mfishtoolspy.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(mft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Parm1 with average cluster distance 0.193 [12].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Npy2r with average cluster distance 0.169 [13].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Hopx with average cluster distance 0.153 [14].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Luzp2 with average cluster distance 0.140 [15].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Cd24a with average cluster distance 0.129 [16].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Ppapdc1a with average cluster distance 0.120 [17].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Ramp1 with average cluster distance 0.114 [18].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Reln with average cluster distance 0.109 [19].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Lypd1 with average cluster distance 0.104 [20].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinho.kim\\Anaconda3\\envs\\mfish\\lib\\site-packages\\distributed\\client.py:3362: UserWarning: Sending large graph of size 24.31 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Rgs12 with average cluster distance 0.100 [21].\n"
     ]
    }
   ],
   "source": [
    "run_mappings = keep_mappings + other_mappings\n",
    "keep_samples = annotation[annotation['mapping_label'].isin(run_mappings)].index.values\n",
    "\n",
    "\n",
    "built_panel, metric = mft.build_mapping_based_marker_panel(\n",
    "    map_data=tpm_log2.loc[run_genes, keep_samples],\n",
    "    mapping_median_data=tpm_median_per_mapping.loc[run_genes, run_mappings],\n",
    "    mapping_call=annotation.loc[keep_samples, 'mapping_label'],\n",
    "    mapping_to_group=annotation.loc[keep_samples, ['mapping_label','group_label']].drop_duplicates().set_index('mapping_label').loc[run_mappings]['group_label'],\n",
    "    group_median_data=tpm_median_per_group.loc[run_genes, keep_groups + other_groups],\n",
    "    panel_size=ops['full_panel_size'],\n",
    "    num_subsample=ops['num_subsample'],\n",
    "    current_panel=ops['starting_genes'].copy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mappings = keep_mappings + other_mappings\n",
    "keep_samples = annotation[annotation['mapping_label'].isin(run_mappings)].index.values\n",
    "\n",
    "built_panel, metric = mft.build_mapping_based_marker_panel(\n",
    "    map_data=tpm_log2.loc[run_genes, keep_samples],\n",
    "    mapping_median_data=tpm_median_per_mapping.loc[run_genes, run_mappings],\n",
    "    mapping_call=annotation.loc[keep_samples, 'mapping_label'],\n",
    "    mapping_to_group=annotation.loc[keep_samples, ['mapping_label','group_label']].drop_duplicates().set_index('mapping_label').loc[run_mappings]['group_label'],\n",
    "    group_median_data=tpm_median_per_group.loc[run_genes, keep_groups + other_groups],\n",
    "    panel_size=ops['full_panel_size'],\n",
    "    num_subsample=ops['num_subsample'],\n",
    "    current_panel=ops['starting_genes'].copy(),\n",
    "    use_parallel=False,\n",
    "    num_iter_each_addition=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the panel and option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "save_ops_fn_base = f'ops_{date}_*.json'\n",
    "save_results_fn_base = f'results_{date}_*.json'\n",
    "\n",
    "prev_ops_fn_list = glob(str(output_folder / save_ops_fn_base))\n",
    "prev_results_fn_list = glob(str(output_folder / save_results_fn_base))\n",
    "\n",
    "if (len(prev_ops_fn_list) == 0) and (len(prev_results_fn_list) == 0):\n",
    "    max_prev_num = -1\n",
    "else:\n",
    "    max_prev_num = max([int(fn.split('.')[0].split('_')[-1]) for fn in prev_ops_fn_list] +\n",
    "                        [int(fn.split('.')[0].split('_')[-1]) for fn in prev_results_fn_list])\n",
    "curr_num = max_prev_num + 1\n",
    "\n",
    "save_ops_fn = output_folder / f'ops_{date}_{curr_num:02}.json'\n",
    "save_results_fn = output_folder / f'results_{date}_{curr_num:02}.json'\n",
    "\n",
    "results = {'run_genes': run_genes,\n",
    "           'built_panel': built_panel,\n",
    "           'metric': metric}\n",
    "\n",
    "\n",
    "with open(save_ops_fn, 'w') as f:\n",
    "    json.dump(ops, f)\n",
    "with open(save_results_fn, 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
